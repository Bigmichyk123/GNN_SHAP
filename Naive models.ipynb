{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"190cqv6aJTL_DFOTbtnviO3Rro1K5agCz","timestamp":1715443094068}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Notes\n","\n","You dont need to look at all full linear models because they yield the same shap values (as they should be because those are just different transformations of the same features) The meta_onehot I supposed it to be some kind of meta data that cannot provide any help for forecasting but I still did it just in case."],"metadata":{"id":"NG3Y9wVh8gCS"}},{"cell_type":"markdown","source":["## Naive forecast"],"metadata":{"id":"v3dLrSZF8Gwy"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Load the .npz file\n","dataset_path = '/content/COVID-US-51x1-20191114-20200531.npz'\n","data = np.load(dataset_path)\n","\n","# List all arrays contained within the .npz file\n","arrays_in_dataset = list(data.keys())\n","\n","# Display the names of the arrays\n","arrays_in_dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JlwGBvBy108I","executionInfo":{"status":"ok","timestamp":1715444063841,"user_tz":240,"elapsed":114,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}},"outputId":"b56b91eb-1d8c-4184-dca1-c1c3747aa5f8"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['poi', 'meta_onehot', 'tcov_relu', 'tcov_half', 'tcov_tanh', 'mask']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["poi_data = data['poi']\n"],"metadata":{"id":"hEX1HbeQ3sjA","executionInfo":{"status":"ok","timestamp":1715444067182,"user_tz":240,"elapsed":311,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["latest_observation = poi_data[-1, :, :]"],"metadata":{"id":"Sb09pmeH3k9J","executionInfo":{"status":"ok","timestamp":1715444067650,"user_tz":240,"elapsed":98,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Determine the split point (e.g., last 20% of the data for testing)\n","split_point = int(poi_data.shape[0] * 0.8)\n","train_data = poi_data[:split_point, :, :]\n","test_data = poi_data[split_point:, :, :]\n","\n","# Use the last observation from the training data as the prediction for all test data slots\n","latest_observation = train_data[-1, :, :]\n","predictions = np.repeat(latest_observation[np.newaxis, :, :], test_data.shape[0], axis=0)\n"],"metadata":{"id":"6IrUQjN54j8X","executionInfo":{"status":"ok","timestamp":1715444068208,"user_tz":240,"elapsed":2,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import mean_absolute_error\n","\n","# testing\n","actual_test_data = poi_data[-1, :, :]\n","\n","# Predictions are the latest observation from the training set\n","predictions = latest_observation  # From the previous step\n","\n","# Calculate MAE\n","mae = mean_absolute_error(actual_test_data.flatten(), predictions.flatten())\n","print(f\"Mean Absolute Error: {mae}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4eWgiY14T9m","executionInfo":{"status":"ok","timestamp":1715444070847,"user_tz":240,"elapsed":1698,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}},"outputId":"e7ed4f43-f896-4847-faf2-85d7bcfa39b7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error: 89.52745098039216\n"]}]},{"cell_type":"markdown","source":["## Historical Average"],"metadata":{"id":"9ucAhvE9NRIh"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import mean_absolute_error\n","\n","# Assume poi_data is a 3D array with dimensions [time, spatial_dim1, spatial_dim2]\n","# And you want to forecast the entire test period with the historical average\n","\n","# Split the data into training and testing\n","split_point = int(poi_data.shape[0] * 0.8)\n","train_data = poi_data[:split_point, :, :]\n","\n","# Compute the historical average\n","# Here, we're computing the simple average across all time points for simplicity\n","historical_average = np.mean(train_data, axis=0)\n","\n","# Prepare the historical average predictions for the test set\n","test_data = poi_data[split_point:, :, :]\n","predictions = np.repeat(historical_average[np.newaxis, :, :], test_data.shape[0], axis=0)\n"],"metadata":{"id":"LDgzuoI6NUF4","executionInfo":{"status":"ok","timestamp":1715444071544,"user_tz":240,"elapsed":105,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Flatten both the predictions and actual test data to compute MAE\n","mae = mean_absolute_error(test_data.flatten(), predictions.flatten())\n","print(f\"Mean Absolute Error (Historical Average): {mae}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtYGEvBeNU62","executionInfo":{"status":"ok","timestamp":1715444073445,"user_tz":240,"elapsed":107,"user":{"displayName":"Yuqi He","userId":"00825545585545104666"}},"outputId":"056638c4-5240-4cfb-de8c-4c9d672612c2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error (Historical Average): 1718.695651674624\n"]}]}]}